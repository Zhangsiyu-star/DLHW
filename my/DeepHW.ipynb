{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解析JSON文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析JSON文件\n",
    "def parseJSON(selectedAttrs,jsonPath):\n",
    "    \"\"\"\n",
    "        selectedAttrs:选择的属性\n",
    "        jsonPath:json文件路径\n",
    "        return:\n",
    "            img_name_list:根据jsonPath返回train或者test下的所有素描图片名字列表  [sketch1_sketch0110.png,...]\n",
    "            attrs_list:每个素描图片对应的属性列表   [[0,0,1,..],[0,0,1,..],...]     [[样本1特征1,样本1特征2,...]，[样本2特征1,样本2特征2,...],...]\n",
    "    \"\"\"    \n",
    "    # 读文件\n",
    "    fp = open(jsonPath, 'r')\n",
    "    data = json.load(fp)\n",
    "    # 素描图片名字列表\n",
    "    img_name_list = list()\n",
    "    # 素描图片对应属性列表\n",
    "    attrs_list = list()\n",
    "    # 每个item是一张图片以及它的属性信息\n",
    "    for item in data:\n",
    "        # photo1/image0001 ==> sketch1_sketch0001.png \n",
    "        str = item['image_name'].replace('photo', 'sketch').replace(\"/\",\"_\").replace('image', 'sketch')\n",
    "        str += '.png'\n",
    "        img_name_list.append(str)\n",
    "        itemAttrs = list()\n",
    "        for attr in selectedAttrs:\n",
    "            itemAttrs.append(item[attr])\n",
    "        attrs_list.append(itemAttrs)\n",
    "    return img_name_list, attrs_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058\n",
      "1058\n"
     ]
    }
   ],
   "source": [
    "# 测试json文件是否能正常解析\n",
    "# selectedAttrs=['hair',\"gender\"]\n",
    "# json_train_path='/home/lab401/zsy/data/FS2K/anno_train.json'\n",
    "# json_test_path='/home/lab401/zsy/data/FS2K/anno_test.json'\n",
    "\n",
    "# img_name_list, attrs_list = parseJSON(selectedAttrs,json_train_path)\n",
    "# print(len(img_name_list))\n",
    "# print(attrs_list)\n",
    "# print(len(attrs_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset：数据集类FS2K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集素描图像的目录\n",
    "trainDirOfImgs='/home/lab401/zsy/data/FS2K/train/sketch'\n",
    "# 测试集素描图像的目录\n",
    "testDirOfImgs='/home/lab401/zsy/data/FS2K/test/sketch'\n",
    "\n",
    "# 加载FS2K数据集的类Dataset\n",
    "class FS2K(Dataset):\n",
    "    def __init__(self,selectedAttrs,jsonPath,transform,mode=\"train\"):\n",
    "        self.img_name_list, self.labels_list = parseJSON(selectedAttrs, jsonPath)\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_name_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_name_list[index]\n",
    "        img_labels = self.labels_list[index]\n",
    "        if(self.mode==\"train\"):\n",
    "            img_path = os.path.join(trainDirOfImgs, img_name)\n",
    "        if(self.mode==\"test\"):\n",
    "            img_path = os.path.join(testDirOfImgs, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform != None:\n",
    "            image = self.transform(image)\n",
    "        return image, img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 测试Dataset是否正常\n",
    "# data = FS2K([\"hair\", \"gender\", \"earring\", \"smile\", \"frontal_face\", \"style\"],\n",
    "#             '/home/lab401/zsy/data/FS2K/anno_train.json',\n",
    "#             transform = transforms.Compose([\n",
    "#                         # Resize成正方形\n",
    "#                         transforms.Resize((224,224)),\n",
    "#                         # 变为tensor变量\n",
    "#                         transforms.ToTensor(),\n",
    "#                         # 进行标准化(标准化就是要把图片3个通道中的数据整理到[-1, 1]区间)\n",
    "#                         transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])\n",
    "#                     ]))\n",
    "# data = FS2K([\"hair\", \"gender\", \"earring\", \"smile\", \"frontal_face\", \"style\"],\n",
    "#                '/home/lab401/zsy/data/FS2K/anno_test.json',transform = transforms.Compose([\n",
    "#                            # Resize成正方形\n",
    "#                            transforms.Resize((224,224)),\n",
    "#                            # 变为tensor变量\n",
    "#                            transforms.ToTensor(),\n",
    "#                            # 进行标准化(标准化就是要把图片3个通道中的数据整理到[-1, 1]区间)\n",
    "#                            transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])\n",
    "#                        ]),\"test\")\n",
    "# print(data[1][0].shape) # torch.Size([3, 224, 224])  \n",
    "# print(data[1][1])       # [0, 0, 1, 1, 1, 0]\n",
    "# print(len(data))        # 1058    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "def get_loader(selectedAttrs,jsonPath,batch_size,transform=None,mode=\"train\"):\n",
    "    dataset = FS2K(selectedAttrs, jsonPath, transform, mode)\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=(mode == 'train'),\n",
    "                             drop_last=True)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试Dataloader是否正常\n",
    "# test_loader = get_loader([\"hair\", \"gender\", \"earring\", \"smile\", \"frontal_face\", \"style\"],\n",
    "#                          '/home/lab401/zsy/data/FS2K/anno_train.json',\n",
    "#                          16,\n",
    "#                          transform = transforms.Compose([\n",
    "#                         # Resize成正方形\n",
    "#                         transforms.Resize((224,224)),\n",
    "#                         # 变为tensor变量\n",
    "#                         transforms.ToTensor(),\n",
    "#                         # 进行标准化(标准化就是要把图片3个通道中的数据整理到[-1, 1]区间)\n",
    "#                         transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])\n",
    "#                     ]))\n",
    "# for data in test_loader:\n",
    "#     imgs,targets = data\n",
    "#     # print(imgs.shape)     # torch.Size([16, 3, 224, 224])  \n",
    "#     print(targets)        \n",
    "#     # [ \n",
    "#     #   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), \n",
    "#     #   tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0]), \n",
    "#     #   tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1]), \n",
    "#     #   tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]), \n",
    "#     #   tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), \n",
    "#     #   tensor([1, 1, 0, 1, 2, 1, 2, 2, 1, 1, 2, 0, 2, 0, 0, 1])\n",
    "#     # ]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型结构（卷积层）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtraction, self).__init__()\n",
    "        # The arguments for commonly used modules:\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "            \n",
    "        # 卷积层\n",
    "        # input image size: [3, 128, 128]\n",
    "        # output image size: [256, 8, 8]\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            # 64 * 126 * 126\n",
    "            # 卷积层之后总会添加BatchNorm2d进行数据的归一化处理，这使得数据在进行Relu之前不会因为数据过大而导致网络性能的不稳定。\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            # 64 * 63 * 63\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            # 128 * 61 * 61\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            # 128 * 31 * 31\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            # 256 * 29 * 29\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4, 4, 0),\n",
    "             # 256 * 8 * 8\n",
    "        )\n",
    "        \n",
    "    def forward(self, image):\n",
    "        return self.cnn_layers(image) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型结构（全连接层）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullConnect(nn.Module):\n",
    "   def __init__(self, input_dim=256 * 8 * 8, output_dim=2):\n",
    "      super(FullConnect, self).__init__()\n",
    "      \n",
    "      # 全连接层\n",
    "      # input image size: [256 * 8 * 8]\n",
    "      # output image size:[2]\n",
    "      self.fc_layers = nn.Sequential(\n",
    "         nn.Linear(input_dim, 256),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(256, 128),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(128, 32),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(32, 8),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(8, output_dim)\n",
    "      )\n",
    "      # self.sigmoid = nn.Sigmoid()\n",
    "         \n",
    "   def forward(self, x):\n",
    "      # 从1维展开\n",
    "      x = x.flatten(1)\n",
    "      return self.fc_layers(x) \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多属性分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MutiClassifier, self).__init__()\n",
    "        \n",
    "        self.featureExtractor = FeatureExtraction()\n",
    "        \n",
    "        self.FC_hair = FullConnect()\n",
    "        self.FC_gender = FullConnect()\n",
    "        self.FC_earring = FullConnect()\n",
    "        self.FC_smile = FullConnect()\n",
    "        self.FC_frontal = FullConnect()\n",
    "        self.FC_style = FullConnect(output_dim=3)\n",
    "\n",
    "        \n",
    "    def forward(self, image):   \n",
    "        # 先经过卷积层提取特征  output image size:256 * 8 * 8\n",
    "        features = self.featureExtractor(image)\n",
    "        # 不同全连接层对不同属性进行分类\n",
    "        hair = self.FC_hair(features)\n",
    "        gender = self.FC_gender(features)\n",
    "        earring = self.FC_earring(features)\n",
    "        smile = self.FC_smile(features)\n",
    "        frontal = self.FC_frontal(features)\n",
    "        style = self.FC_style(features)\n",
    "        \n",
    "        # hair:[0,1]  gender:[0,1]  earring:[0,1]  smile:[0,1]  frontal:[0,1]   style:[0,1,2]   \n",
    "        return hair, gender, earring, smile, frontal, style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {  \n",
    "    # 训练集的json文件路径\n",
    "    \"json_train_path\":'/home/lab401/zsy/data/FS2K/anno_train.json',\n",
    "    # 测试集的json文件路径\n",
    "    \"json_test_path\":'/home/lab401/zsy/data/FS2K/anno_test.json',\n",
    "    # 选择的属性\n",
    "    # \"selectedAttrs\":[\"hair\",\"gender\",\"earring\",\"smile\",\"frontal_face\",\"style\"],\n",
    "    \"selectedAttrs\":[\"hair\"],\n",
    "    # 模型保存路径\n",
    "    'save_path': '/home/lab401/zsy/DeepHW/model/model.pth', \n",
    "    # 超参\n",
    "    \"Epoches\":50,\n",
    "    \"batch_szie\":16,\n",
    "    \"lr\":0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前置操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理需要用到的变量\n",
    "Epoches = config[\"Epoches\"]\n",
    "batch_size = config[\"batch_szie\"]\n",
    "learning_rate = config[\"lr\"]\n",
    "selectedAttrs = config[\"selectedAttrs\"]\n",
    "json_train_path = config[\"json_train_path\"]\n",
    "json_test_path = config[\"json_test_path\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "transform = transforms.Compose([\n",
    "    # Resize成正方形\n",
    "    transforms.Resize((128, 128)),\n",
    "    # 变为tensor变量\n",
    "    transforms.ToTensor(),\n",
    "    # 进行标准化(标准化就是要把图片3个通道中的数据整理到[-1, 1]区间)\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])\n",
    "])\n",
    "train_loader = get_loader(selectedAttrs,json_train_path,batch_size,transform)\n",
    "test_loader = get_loader(selectedAttrs,json_test_path,batch_size,transform,\"test\")\n",
    "# 初始化模型，并将其放在指定的设备上\n",
    "model = MutiClassifier().to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lossOfGender' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_437864/2166092158.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0;31m# lossOfStyle = F.cross_entropy(style,labels[5].to(device))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m         \u001B[0;31m# 计算总损失\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m         \u001B[0mbatch_total_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlossOfHair\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlossOfGender\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlossOfEarring\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlossOfSmile\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlossOfFrontal\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlossOfStyle\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m         \u001B[0;31m# 应首先清除上一步中存储在参数中的梯度\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'lossOfGender' is not defined"
     ]
    }
   ],
   "source": [
    "# 记录训练每个epoch的平均loss\n",
    "train_loss_record = []\n",
    "# 记录验证每个epoch的平均acc\n",
    "valid_acc_record = []\n",
    "# 记录验证每个epoch的每个属性的acc    \n",
    "valid_acc_item_record = {}\n",
    "# 验证集上最佳准确率        \n",
    "best_acc = 0  \n",
    "\n",
    "# 初始化\n",
    "for attr in selectedAttrs:\n",
    "    valid_acc_item_record[attr] = []\n",
    "\n",
    "for epoch in range(Epoches):\n",
    "    # ---------- Train Start ---------- \n",
    "    \n",
    "    # 设置模型为训练模式\n",
    "    model.train()\n",
    "    \n",
    "    # 记录训练单个epoch的loss\n",
    "    batch_loss = 0\n",
    "    \n",
    "    for batch_idx,data in enumerate(train_loader):\n",
    "        imgs, labels = data\n",
    "        # 将素描图片放入模型，得出预测值\n",
    "        hair, gender, earring, smile, frontal, style = model(imgs.to(device))\n",
    "        # 计算各个属性的交叉熵损失\n",
    "        # F.cross_entropy(input, target):input的维度为[batchsize,classes,width,height]，target的维度为[batchsize,width,height]。\n",
    "        lossOfHair = F.cross_entropy(hair,labels[0].to(device))\n",
    "        # lossOfGender = F.cross_entropy(gender,labels[1].to(device))\n",
    "        # lossOfEarring = F.cross_entropy(earring,labels[2].to(device))\n",
    "        # lossOfSmile = F.cross_entropy(smile,labels[3].to(device))\n",
    "        # lossOfFrontal = F.cross_entropy(frontal,labels[4].to(device))\n",
    "        # lossOfStyle = F.cross_entropy(style,labels[5].to(device))\n",
    "        # 计算总损失\n",
    "        batch_total_loss = lossOfHair + lossOfGender + lossOfEarring + lossOfSmile + lossOfFrontal + lossOfStyle\n",
    "        # 应首先清除上一步中存储在参数中的梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 梯度反传\n",
    "        batch_total_loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        # 累加这个batch的loss\n",
    "        batch_loss += batch_total_loss.item()  \n",
    "    \n",
    "    # 计算一个epoch的平均损失 epoch_loss\n",
    "    epoch_average_loss = batch_loss / (batch_idx + 1)\n",
    "    # 记录每个epoch的平均损失\n",
    "    train_loss_record.append(epoch_average_loss)\n",
    "    # 打印loss信息\n",
    "    print(\"Epoch: %d/%d, loss: %.4f\" % (epoch, Epoches, epoch_average_loss))    \n",
    "    \n",
    "    # ---------- Train End ---------- \n",
    "    \n",
    "    # ---------- Valid Start ---------- \n",
    "    \n",
    "    # 调整为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 统计单个epoch正确率\n",
    "    correct_dict = {}\n",
    "    # 保存单个epoch预测值\n",
    "    predict_dict = {} \n",
    "    # 保存单个epoch label \n",
    "    label_dict = {} \n",
    "    \n",
    "    # 初始化\n",
    "    for attr in selectedAttrs:\n",
    "        correct_dict[attr] = 0\n",
    "        \n",
    "    # 批量迭代验证集\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        imgs, labels = data\n",
    "        # 在验证集上不需要计算梯度\n",
    "        with torch.no_grad():\n",
    "            hair, gender, earring, smile, frontal, style = model(imgs.to(device))\n",
    "            # 用于存放模型预测batch_size个样本各个属性的数据\n",
    "            out_dict = {'hair': hair, 'gender': gender, 'earring': earring,\n",
    "                        'smile': smile, 'frontal_face': frontal, 'style': style}\n",
    "            # 一个batch包含的样本数\n",
    "            batch_len = len(out_dict['hair'])\n",
    "            # 计算准确率（比较batch中每个样本每个属性）\n",
    "            # i表示第几个样本\n",
    "            for i in range(batch_len):\n",
    "                # 取出selectedAttrs中每一个选中的属性，以及其index\n",
    "                for attr_idx, attr in enumerate(selectedAttrs):\n",
    "                    # out_dict[attr]：取出某个属性batch_size个预测值\n",
    "                    # out_dict[attr][i]：选择out_dict中attr属性的第i个样本的预测值（[0.2,0.8]）\n",
    "                    # np.argmax(out_dict[attr][i].data.cpu().numpy())：通过argmax获得下标0或1或2\n",
    "                    # 第i个样本attr属性值的预测值\n",
    "                    pred = np.argmax(out_dict[attr][i].data.cpu().numpy())  \n",
    "                    # labels[attr_idx]：batch_size个样本的attr属性\n",
    "                    # labels[attr_idx].data.cpu().numpy()[i]：取出第i个样本的attr属性label值\n",
    "                    # 第i个样本每个属性值的label值\n",
    "                    true_label = labels[attr_idx].data.cpu().numpy()[i]  \n",
    "                    # 判断第i个样本的预测值和label值是否相等\n",
    "                    if pred == true_label:\n",
    "                        # 如果相等则表示第i个样本的attr属性预测正确，该属性预测正确数加1\n",
    "                        correct_dict[attr] = correct_dict[attr] + 1\n",
    "    \n",
    "    # 用于记录平均准确率 每个epoch后所有样本所有属性的总准确率 / 属性数\n",
    "    valid_average_acc = 0\n",
    "    # 计算每个epoch后每个属性的准确率(80 --> 80%)\n",
    "    for attr in selectedAttrs:\n",
    "        correct_dict[attr] = correct_dict[attr] * 100 / (len(test_loader) * batch_size)\n",
    "        # 记录验证每个epoch每个属性准确率\n",
    "        valid_acc_item_record[attr].append(correct_dict[attr])\n",
    "        valid_average_acc += correct_dict[attr]\n",
    "    valid_average_acc /= len(selectedAttrs)\n",
    "    # 记录验证每个epoch平均准确率\n",
    "    valid_acc_record.append(valid_average_acc)\n",
    "    \n",
    "    # ---------- Valid End ---------- \n",
    "    \n",
    "    # ---------- Chase Best Model Start ---------- \n",
    "                  \n",
    "    # 比较正确率并保存最佳模型\n",
    "    if valid_average_acc > best_acc:\n",
    "        best_acc = valid_average_acc\n",
    "        # 将最好的模型参数保存到指定路径\n",
    "        torch.save(model.state_dict(), config['save_path'])  \n",
    "        print(\"Epoch: %d/%d, Best_acc: %.4f\" % (epoch, Epoches, best_acc))         \n",
    "        \n",
    "    # ---------- Chase Best Model End ---------- \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画图观察训练集上loss、acc的变化以及测试集上loss、acc的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "def plot_loss_curve(train_loss_record,valid_loss_record):\n",
    "    train_x = range(len(train_loss_record))\n",
    "    valid_x = range(len(valid_loss_record))\n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(train_x, train_loss_record, linestyle=\"-.\", c='tab:red', label='train_loss')\n",
    "    plt.plot(valid_x, valid_loss_record, linestyle=\"-.\", c='tab:green', label='valid_loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_curve(train_loss,valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_curve(train_accs_record,valid_accs_record):\n",
    "    train_x = range(len(train_accs_record))\n",
    "    valid_x = range(len(valid_accs_record))\n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(train_x, train_accs_record, linestyle=\"-.\", c='tab:red', label='train_acc')\n",
    "    plt.plot(valid_x, valid_accs_record, linestyle=\"-.\", c='tab:green', label='valid_acc')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "train_accs_temp = []\n",
    "valid_accs_temp = []\n",
    "for i in train_accs:\n",
    "    train_accs_temp.append(i.cpu().numpy())\n",
    "for i in valid_accs:\n",
    "    valid_accs_temp.append(i.cpu().numpy())\n",
    "    \n",
    "plot_acc_curve(train_accs_temp,valid_accs_temp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fcb8cad015ff4636caa2b8d2e37ccaf5edab670c59c7dcbd43339b3ab0787a0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('zsy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}